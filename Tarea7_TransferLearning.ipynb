{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea7_TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karlacuv/MCD_Procesamiento/blob/main/Tarea7_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning\n",
        "### Abril Grisel Guevara Cedillo - 1419239\n",
        "### Kin Michelle Neváres Ríos - 1671272\n",
        "### Karla Cureño Vega - 2085376"
      ],
      "metadata": {
        "id": "aeMwFyrfI8gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13J7m-hDJAP0",
        "outputId": "6a8f2a4f-4815-4f4d-9f4b-95f67958694b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/My Drive/Files/MCD/Procesamiento y Clasificación de Datos/Base Miniproyecto 2/archive.zip'"
      ],
      "metadata": {
        "id": "A8x4mYFaJgzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09717895-f78f-42d2-b6a1-614e686d4a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/Files/MCD/Procesamiento y Clasificación de Datos/Base Miniproyecto 2/archive.zip\n",
            "replace test/angry/PrivateTest_10131363.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vgg16 model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# load model\n",
        "model = VGG16()\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXC_8RyPMFBs",
        "outputId": "fbfc9e60-4d99-44fa-e908-45f7d8b60ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como primer paso se importa la libreria glob la cual nos ayudara a importar las imagenes por categoria, y modificarlas para poder usar el transfer learning.\n",
        "\n",
        "Como regla inicial para todas las arquitecturas, el tamano de las imagenes debe de ser 224x224."
      ],
      "metadata": {
        "id": "HprX4t0UOdKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For training set only\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "angry = glob.glob('train/angry/*.*')\n",
        "disgust = glob.glob('train/disgust/*.*')\n",
        "fear = glob.glob('train/fear/*.*')\n",
        "happy = glob.glob('train/happy/*.*')\n",
        "neutral = glob.glob('train/neutral/*.*')\n",
        "sad = glob.glob('train/sad/*.*')\n",
        "surprise = glob.glob('train/surprise/*.*')\n",
        "data = []\n",
        "labels = []\n",
        "for i in angry:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Angry')\n",
        "for i in disgust:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Disgust')\n",
        "for i in fear:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Fear')\n",
        "for i in happy:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Happy')\n",
        "for i in neutral:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Neutral')\n",
        "for i in sad:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Sad')\n",
        "for i in surprise:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Surprise')\n",
        "train_data = np.array(data)\n",
        "train_labels = np.array(labels)"
      ],
      "metadata": {
        "id": "-ArI39g9MqdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtsEWyYnRNhB",
        "outputId": "b8243371-912e-4cdf-b2aa-e1d125cf2463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28709, 48, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For test set\n",
        "\n",
        "angry = glob.glob('test/angry/*.*')\n",
        "disgust = glob.glob('test/disgust/*.*')\n",
        "fear = glob.glob('test/fear/*.*')\n",
        "happy = glob.glob('test/happy/*.*')\n",
        "neutral = glob.glob('test/neutral/*.*')\n",
        "sad = glob.glob('test/sad/*.*')\n",
        "surprise = glob.glob('test/surprise/*.*')\n",
        "data = []\n",
        "labels = []\n",
        "for i in angry:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Angry')\n",
        "for i in disgust:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Disgust')\n",
        "for i in fear:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Fear')\n",
        "for i in happy:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Happy')\n",
        "for i in neutral:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Neutral')\n",
        "for i in sad:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Sad')\n",
        "for i in surprise:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i, color_mode='grayscale')\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append('Surprise')\n",
        "test_data = np.array(data)\n",
        "test_labels = np.array(labels)"
      ],
      "metadata": {
        "id": "at5GRTkOPIgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXsQKhP1RU26",
        "outputId": "8150b5d2-0e2b-4ba0-9c55-24526f1ed4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7178, 48, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos los datos y hacemos una transformación a las etiquetas que pusimos anteriormente."
      ],
      "metadata": {
        "id": "tkYPJIdoOhnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "X_train = train_data.astype('float32')\n",
        "X_test = test_data.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "lb = LabelEncoder()\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(train_labels))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(test_labels))"
      ],
      "metadata": {
        "id": "Db2T23gZQUeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = VGG16(weights=None,include_top=False, input_shape=(48, 48, 1))\n",
        "\n",
        "for layer in vgg_model.layers:\n",
        "  layer.trainable = False\n",
        "# Make sure you have frozen the correct layers\n",
        "for i, layer in enumerate(vgg_model.layers):\n",
        "    print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbAsj4yOWKDt",
        "outputId": "59b98c92-3a43-4c23-f995-60054e5a4d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_6 False\n",
            "1 block1_conv1 False\n",
            "2 block1_conv2 False\n",
            "3 block1_pool False\n",
            "4 block2_conv1 False\n",
            "5 block2_conv2 False\n",
            "6 block2_pool False\n",
            "7 block3_conv1 False\n",
            "8 block3_conv2 False\n",
            "9 block3_conv3 False\n",
            "10 block3_pool False\n",
            "11 block4_conv1 False\n",
            "12 block4_conv2 False\n",
            "13 block4_conv3 False\n",
            "14 block4_pool False\n",
            "15 block5_conv1 False\n",
            "16 block5_conv2 False\n",
            "17 block5_conv3 False\n",
            "18 block5_pool False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras import Model\n",
        "\n",
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(7, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "pU6DdJdpWeid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "learning_rate= 5e-5\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
        "history = transfer_model.fit(X_train, y_train, batch_size = 100, callbacks=[callback], epochs=50, validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YscrDXaW3NL",
        "outputId": "f0c17049-d813-4ab9-98af-1c576204946a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "288/288 [==============================] - 51s 175ms/step - loss: 1.8405 - accuracy: 0.2513 - val_loss: 1.8418 - val_accuracy: 0.2471\n",
            "Epoch 2/50\n",
            "288/288 [==============================] - 40s 138ms/step - loss: 1.8390 - accuracy: 0.2513 - val_loss: 1.8404 - val_accuracy: 0.2471\n",
            "Epoch 3/50\n",
            "288/288 [==============================] - 40s 139ms/step - loss: 1.8376 - accuracy: 0.2513 - val_loss: 1.8392 - val_accuracy: 0.2471\n",
            "Epoch 4/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8363 - accuracy: 0.2513 - val_loss: 1.8380 - val_accuracy: 0.2471\n",
            "Epoch 5/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8351 - accuracy: 0.2513 - val_loss: 1.8369 - val_accuracy: 0.2471\n",
            "Epoch 6/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8340 - accuracy: 0.2513 - val_loss: 1.8358 - val_accuracy: 0.2471\n",
            "Epoch 7/50\n",
            "288/288 [==============================] - 41s 142ms/step - loss: 1.8329 - accuracy: 0.2513 - val_loss: 1.8348 - val_accuracy: 0.2471\n",
            "Epoch 8/50\n",
            "288/288 [==============================] - 40s 138ms/step - loss: 1.8319 - accuracy: 0.2513 - val_loss: 1.8339 - val_accuracy: 0.2471\n",
            "Epoch 9/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8309 - accuracy: 0.2513 - val_loss: 1.8330 - val_accuracy: 0.2471\n",
            "Epoch 10/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8300 - accuracy: 0.2513 - val_loss: 1.8321 - val_accuracy: 0.2471\n",
            "Epoch 11/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8291 - accuracy: 0.2513 - val_loss: 1.8313 - val_accuracy: 0.2471\n",
            "Epoch 12/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8283 - accuracy: 0.2513 - val_loss: 1.8306 - val_accuracy: 0.2471\n",
            "Epoch 13/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8275 - accuracy: 0.2513 - val_loss: 1.8299 - val_accuracy: 0.2471\n",
            "Epoch 14/50\n",
            "288/288 [==============================] - 40s 138ms/step - loss: 1.8268 - accuracy: 0.2513 - val_loss: 1.8292 - val_accuracy: 0.2471\n",
            "Epoch 15/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8261 - accuracy: 0.2513 - val_loss: 1.8285 - val_accuracy: 0.2471\n",
            "Epoch 16/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8254 - accuracy: 0.2513 - val_loss: 1.8279 - val_accuracy: 0.2471\n",
            "Epoch 17/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8247 - accuracy: 0.2513 - val_loss: 1.8273 - val_accuracy: 0.2471\n",
            "Epoch 18/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8241 - accuracy: 0.2513 - val_loss: 1.8267 - val_accuracy: 0.2471\n",
            "Epoch 19/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8235 - accuracy: 0.2513 - val_loss: 1.8262 - val_accuracy: 0.2471\n",
            "Epoch 20/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8230 - accuracy: 0.2513 - val_loss: 1.8257 - val_accuracy: 0.2471\n",
            "Epoch 21/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8225 - accuracy: 0.2513 - val_loss: 1.8252 - val_accuracy: 0.2471\n",
            "Epoch 22/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8219 - accuracy: 0.2513 - val_loss: 1.8247 - val_accuracy: 0.2471\n",
            "Epoch 23/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8214 - accuracy: 0.2513 - val_loss: 1.8243 - val_accuracy: 0.2471\n",
            "Epoch 24/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8210 - accuracy: 0.2513 - val_loss: 1.8238 - val_accuracy: 0.2471\n",
            "Epoch 25/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8205 - accuracy: 0.2513 - val_loss: 1.8234 - val_accuracy: 0.2471\n",
            "Epoch 26/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8201 - accuracy: 0.2513 - val_loss: 1.8230 - val_accuracy: 0.2471\n",
            "Epoch 27/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8197 - accuracy: 0.2513 - val_loss: 1.8226 - val_accuracy: 0.2471\n",
            "Epoch 28/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8193 - accuracy: 0.2513 - val_loss: 1.8223 - val_accuracy: 0.2471\n",
            "Epoch 29/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8189 - accuracy: 0.2513 - val_loss: 1.8219 - val_accuracy: 0.2471\n",
            "Epoch 30/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8185 - accuracy: 0.2513 - val_loss: 1.8216 - val_accuracy: 0.2471\n",
            "Epoch 31/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8182 - accuracy: 0.2513 - val_loss: 1.8212 - val_accuracy: 0.2471\n",
            "Epoch 32/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8178 - accuracy: 0.2513 - val_loss: 1.8209 - val_accuracy: 0.2471\n",
            "Epoch 33/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8175 - accuracy: 0.2513 - val_loss: 1.8206 - val_accuracy: 0.2471\n",
            "Epoch 34/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8172 - accuracy: 0.2513 - val_loss: 1.8203 - val_accuracy: 0.2471\n",
            "Epoch 35/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8168 - accuracy: 0.2513 - val_loss: 1.8200 - val_accuracy: 0.2471\n",
            "Epoch 36/50\n",
            "288/288 [==============================] - 50s 174ms/step - loss: 1.8165 - accuracy: 0.2513 - val_loss: 1.8197 - val_accuracy: 0.2471\n",
            "Epoch 37/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8163 - accuracy: 0.2513 - val_loss: 1.8194 - val_accuracy: 0.2471\n",
            "Epoch 38/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8160 - accuracy: 0.2513 - val_loss: 1.8192 - val_accuracy: 0.2471\n",
            "Epoch 39/50\n",
            "288/288 [==============================] - 39s 137ms/step - loss: 1.8157 - accuracy: 0.2513 - val_loss: 1.8189 - val_accuracy: 0.2471\n",
            "Epoch 40/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8155 - accuracy: 0.2513 - val_loss: 1.8187 - val_accuracy: 0.2471\n",
            "Epoch 41/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8152 - accuracy: 0.2513 - val_loss: 1.8184 - val_accuracy: 0.2471\n",
            "Epoch 42/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8150 - accuracy: 0.2513 - val_loss: 1.8182 - val_accuracy: 0.2471\n",
            "Epoch 43/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8148 - accuracy: 0.2513 - val_loss: 1.8180 - val_accuracy: 0.2471\n",
            "Epoch 44/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8145 - accuracy: 0.2513 - val_loss: 1.8178 - val_accuracy: 0.2471\n",
            "Epoch 45/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8143 - accuracy: 0.2513 - val_loss: 1.8176 - val_accuracy: 0.2471\n",
            "Epoch 46/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8141 - accuracy: 0.2513 - val_loss: 1.8174 - val_accuracy: 0.2471\n",
            "Epoch 47/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8139 - accuracy: 0.2513 - val_loss: 1.8172 - val_accuracy: 0.2471\n",
            "Epoch 48/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8137 - accuracy: 0.2513 - val_loss: 1.8170 - val_accuracy: 0.2471\n",
            "Epoch 49/50\n",
            "288/288 [==============================] - 39s 136ms/step - loss: 1.8136 - accuracy: 0.2513 - val_loss: 1.8169 - val_accuracy: 0.2471\n",
            "Epoch 50/50\n",
            "288/288 [==============================] - 50s 175ms/step - loss: 1.8134 - accuracy: 0.2513 - val_loss: 1.8167 - val_accuracy: 0.2471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_model.save('my_model_BS100.h5')"
      ],
      "metadata": {
        "id": "KAoz5EdR4M6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abril: Lectura de todas las imágenes con formato RGB, los pesos de la red se inicializan con ImageNet. La red fue entrenada con un batch_size de 300 y 30 epochs. El accuracy final en el set de validación fue de 0.2471.\n",
        "\n",
        "Karla: Lectura de todas las imágenes con formato escala de grises, los pesos de la red se inicializan de manera aleatoria. La red fue entrenada con un batch_size de 100 y 50 epochs. El accuracy final en el set de validación fue de 0.2471.\n",
        "\n",
        "Kin: Lectura de una muestra de las imágenes para optimizar tiempo de ejcución con formato escala de grises, los pesos de la red se inicializan de manera aleatoria. La red fue entrenada con un batch_size de 1 y 30 epochs. El accuracy final en el set de validación fue de 0.2369.\n",
        "\n",
        "### Conclusión\n",
        "\n",
        "Debido a que las imágenes de nuestro dataset están en escala de grises y la red preentrenada está optimizada para imágenes en RGB, esta red no es adecuada para este set de datos, pues al leer nuestras imágenes con formato RGB, éstas siguen estando en escala de grises.\n",
        "\n",
        "Con la modificación de los pesos para que empezaran de manera aleatoria en vez de con los pesos de ImageNet, hace que sea muy díficil para la red encontrar los pesos adecuados, por lo que la precisión no mejora.\n",
        "\n",
        "Además, el tamaño de las imágenes en el dataset es muy pequeño por lo que tener una red tan densa hace que se pierda mucha información de la imagen en cada capa de la red.\n",
        "\n",
        "Al extraer una muestra de todas las imágenes, el tiempo de ejecución mejora, sin embargo, el \"accuracy\" empeora un poco."
      ],
      "metadata": {
        "id": "z3sPGYvtAuYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3B8aDNh10s7A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}